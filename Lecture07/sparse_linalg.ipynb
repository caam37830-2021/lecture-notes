{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Linear Algebra\n",
    "\n",
    "So far, we have seen how sparse matrices and linear operators can be used to speed up basic matrix-vector and matrix-matrix operations, and decrease the memory footprint of the representation of a linear map.\n",
    "\n",
    "Just as there are special data types for sparse and structured matrices, there are specialized linear algebra routines which allow you to take advantage of sparsity and fast matrix-vector products.\n",
    "\n",
    "Routines for sparse linear algebra are found in `scipy.sparse.linalg`, which we'll import as `sla`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import scipy.sparse as sparse\n",
    "import scipy.sparse.linalg as sla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Direct Methods\n",
    "\n",
    "This typically refers to producing a factorization of a sparse matrix for use in solving linear systems.\n",
    "\n",
    "The thing to keep in mind is that many factorizations will generally be dense, even if the original matrix is sparse.  E.g. eigenvalue decompositions, QR decomposition, SVD, etc.  This means that if we compute a factorization, we are going to lose all the advantages we had from sparsity.  \n",
    "\n",
    "What we really want is a factorization where if `A` is sparse, the terms in the factorization are also sparse.  The factorization where this is easiest to achieve is the LU decomposition.  In general, the `L` and `U` terms will be more dense than `A`, and sometimes much more dense.  However, we can seek a permuted version of the matrix `A` which will minimize the amount of \"fill-in\" which occurs.  This is often done using \"nested disection\" algorithm, which is outside the scope of this course.  If you ever need to do this explicitly, the [METIS package](http://glaros.dtc.umn.edu/gkhome/metis/metis/overview) is commonly used.\n",
    "\n",
    "We'll just use the function [`sla.splu`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.splu.html#scipy.sparse.linalg.splu) (SParse LU) at a high level, which produces a factorization object that can be used to solve linear systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAbAklEQVR4nO2dXYxdV3XHf4sYhwSTkDRNMrFj2SgOklPUQkYRhQpFGIsSR7gPBblSKtOm8ktK+GiFx8UIVbKloUURPFSVLChKCyFxQ9REGBGctHmolI/aFBoSk8YwQ3DixHEFhrQoxs7qwz03uR6fe+/52mevfc76SaOZOXPOnXXP3fu/11p77X1EVXEcp7+8LrYBjuPExUXAcXqOi4Dj9BwXAcfpOS4CjtNzXAQcp+dEFwER+X0ReUpEDovInAF7rhSRfxORQyLyhIh8LDt+sYjsF5Gns+8XjVyzI7P/KRF5fwSbzxGR/xSRbyZg65tF5G4R+WF2j3/Xqr0i8omsDfxARL4uIm+wamstVDXaF3AO8CPgLcBy4PvA+sg2zQDvyH5+E/DfwHrgb4C57Pgc8Lns5/WZ3ecCa7P3c07LNn8SuAP4Zva7ZVtvB/4s+3k58GaL9gIrgQXgvOz3vcBHLNpa9yu2J3AdcFhVf6yqJ4E7gc0xDVLVo6r63eznXwKHGDSIzQwaMNn3P8h+3gzcqaovq+oCcJjB+2oFEVkFbAK+NHLYqq0XAO8BvgygqidV9edW7QWWAeeJyDLgfOA5w7ZWJrYIrAR+OvL7keyYCURkDfB24FHgMlU9CgOhAC7NTov9Hr4AfAp4ZeSYVVvfArwIfCULX74kIm+0aK+qPgt8HngGOAqcUNXvWLS1LrFFQHKOmahjFpEVwDeAj6vqLyadmnOslfcgIjcCx1T1YNFLco61eb+XAe8A/l5V3w78LwOXehwx7+1FDEb3tcAVwBtF5KZJl+QcM9GWpxFbBI4AV478voqByxUVEXk9AwH4mqrekx1+QURmsr/PAMey4zHfw7uBD4rIIoNQ6r0i8lWjtg7//xFVfTT7/W4GomDR3vcBC6r6oqr+GrgHeJdRW2sRWwT+A1gnImtFZDmwBbgvpkEiIgxi1kOqetvIn+4DtmY/bwXuHTm+RUTOFZG1wDrgsTZsVdUdqrpKVdcwuHf/qqo3WbQ1s/d54Kci8tbs0AbgSaP2PgO8U0TOz9rEBgb5IYu21iN2ZhK4gUEG/kfApw3Y83sM3Lj/Ar6Xfd0A/AbwIPB09v3ikWs+ndn/FPCBSHZfz2uzA2ZtBX4HOJDd338BLrJqL/DXwA+BHwD/xCDzb9LWOl+SGe84Tk+JHQ44jhMZFwHH6TkuAo7Tc1wEHKfnBBMBawuDHMfJJ4gIiMg5wN8BH2CwsOKPRGT9hPO3hbAjFCnZm5KtkJa9Kdk6iVCeQNmFQandzJTsTclWSMvelGwdSygRSHYxheP0jWWBXnfqYorMldoG8LrzLrj23Jl1hauW3rbywjN+f/zZExVMLP76S1m9ejWzs7ONVlmVeQ/T7BulKVtD2beUEPc2FKFsrduex93/gwcPHlfV31x6PJQITF1Moap7gD0A586s05mtXyj84seBxflNr/6+Zm5fdUtzODDy2m1R5j24fd1ldtd+Zl46Wes1xt1/EflJ3vFQ4UDwhUFrG+74jmOB4zUFoApBPAFVPSUifw7cz2ALsX9Q1Sca/R8MhGDBRx1zTPIaLlmxnAM7N7ZojTONUOEAqvot4FuhXh9eEwLnNWZ37S80msTqjDFGOmcywUSgLUJkkJaOZNZGr6IdfRJlr79kxXLvwB0leREYZTRZ2ERHGdLE61izpyxLRbDpZKzTHOM+m+WXX3Vt3vFOicBojsDaqGXNHic+TQ4MdeiUCPQhWehueVjq5lTKXG/lc+yUCEA3koWLU0TMXfFwFO2Y486re30MOrmUOIlyM8cxggkReNvKC3PrjB2nTWZ37Y9tQhRMiADAwvwmFwInKpZc9KUszm+aGiZWxYwIAJ1O6DkDLlmxPLYJzhI6lxh0ilG3MxbNblsrtHLOxkWgANZGrxAdsMqctXfwbtB7EUixIQ/tndZxj790kjVz+wq9xyrxsOUYuquEqC/opQgI3cg/pDgn7ZRn1BMdinmZWpFhQlE+d2Pu06t7KQJ9qCx0JlO3ZHdciBhipB56dKHopQhAOSGwvjzXKU/djjrucx49nkplp6kpwrZRBh/UtCIRd7udVCmS1O6tJzBKk503FfW3TFOeV8hVetM+5zZnlIYxf977HQ0lxi0l7rUnMErqi466RFOeV1MCMKzWK1OxF8MrrPo/zYlArDn5Lqw+dJwqmAsHyiTWmna9LcwaWCtMcrqPOREow+L8pqSEINQCkCawtMlFH1naNtrMLSUtAjC4eWvn9jW6h8Bw1sByp4Vy5cOjNJUwc6+lGyQvAjCo/mtaCCB+aDCNcaHT0k5ep9jEuhBapqpIt00nRAAGQhAqNEgNd+ttUKdwrE0B6YwIQLjQwHHaps3K086IQOjtm0cXHTXtcXhZshMTkyJQpVOEdoGHycKqTHLbvCz5TJpyhUPMeFiO86u+X5Mi0IVO0ZWEWoxHsjX1+pNep6qg17UtpNd3YOfGSu/LpAg4dgktvG2FRiFH9BjPiqyDi4BjijJeYB0vJaQ3Y9lDzcPc2gHHqUpqnc8KLgKJUWT/A6e/VAlfPBxIEB/x2iHFqdtJdozbY9A9AccZQxdmqYrgItBBYteiO2kxNRwQkSuBfwQuB14B9qjqF0XkYuAuYA2wCHxYVX+WXbMDuBk4DdyqqvcHsX6EMlM+1pS7ik2jmfGl7ug4l9C3PnPyKJITOAX8hap+V0TeBBwUkf3AR4AHVXVeROaAOWC7iKwHtgDXAFcAD4jI1ap6uqhRVeZwY25GUpcqe8mPUlRALApgV7DWpsowVQRU9ShwNPv5lyJyCFgJbAauz067HXgI2J4dv1NVXwYWROQwcB3wcFGjrCRZ6lBmGXLodQ9DmtiU00ONdmjzPpeaHRCRNcDbgUeByzKBQFWPisil2WkrgUdGLjuSHVv6WtuAbQCrV68ubbh1yuxQZGF0jiW8bQlgahx/6SSzu/a38rkUFgERWQF8A/i4qv5CRMaemnPsrBW5qroH2AMwOzvbyRW7be5ZGKPGvwmaFACrXkrVrcPaEsdCIiAir2cgAF9T1Xuywy+IyEzmBcwAx7LjR4ArRy5fBTzXlMFN0GZsHGvz0i6Orl1ZlFWGNmoViswOCPBl4JCq3jbyp/uArcB89v3ekeN3iMhtDBKD64DHKlkXiLaz5xZ2Me4iKRbzlKWNWoUidQLvBv4YeK+IfC/7uoFB598oIk8DG7PfUdUngL3Ak8C3gVvKzAx0lVS3KgtJ3fLn0B3EanjRNEVmB/6d/DgfYMOYa3YDu2vY1UncIzgT6yFLiP0ILOIVgy3jHoFjDReBCLgQOJbwVYSRqLtnoRMXy3sNlsVFYAQvq3WKkupsQx4uAiMU+WDbGr0XA21vniopjKhFsTbYuAiUJMQDTkIw7DRdmUu3bFtZ6i4YaxpzIpBCow3xyLMmyKuo68vGGH2njqdkTgRSabQWPYK2FpxYIfXkXBn7Q36u5kQgJax5BLGFsSypd+K6WBFsF4GaLAYSghiVhW2vRKz72ql4jdZxEWiAEEJgoaDIO085Ushn5eEiYBhL+YbUmbQnY1Ok6pm4CDgTGefhWBvNymCtE8bGRaACKWyJFdpG6+9/GkNxS1nMmqLzIhAiTkuhA6RgowX8PvVABFKN0xynCE3sEm1uKXHROeGuzh07ThmaGOTMeQJ9j8/q4MLoVMGcCFjDchJQoNdblVlbjZcqLgJTsNzI+r5n4VKv0VIJd0qYywk45bBQWegMSDWflawnYNlNbxtrHkGs8tlY4UGq5cJDkvUEXADOpG2PYNJoFmta9sDOjSzOb2r9SUWpT0Mn6wk4ZzPqEYQaFWM9CiyU52fNNY+Bi0BGVxpD2dAgdljVxv+39AzDacnLaSFDiPvVCxHIawRLb+bxl052JrtcRggO7NwY9X1bdZFjMWyH48SgziPVfjLmb70QgTy63vjKCEHfd/iZRCxPqYn/uXTwk8/kn9dbEegDRYXAYsa6SSaNrNPo+mABCc8OOMXwOoIBfejMVXFPoACTEksp5BGs1RE4tnAR6AkuBOkSOi+RbDiQaolmTNoKDfyzaZbQoUyynoCVZFZTD5BoK6xowyPIe595o1nX4vRxYaP1kDFZEQAbNdtWxKgMMUKDrnX4POrMQsSksAiIyDnAAeBZVb1RRC4G7gLWAIvAh1X1Z9m5O4CbgdPArap6f8N2A+nXbMdEmbzZpgWBLfJ/rI2yTba1Ou+tTKhVxhP4GHAIuCD7fQ54UFXnRWQu+327iKwHtgDXAFcAD4jI1ap6usT/Co4XyLxG3n2IKbCWynxTo8q9KyQCIrIK2ATsBj6ZHd4MXJ/9fDvwELA9O36nqr4MLIjIYeA64OHS1gXEmsvmu+TYpInPxfpnW9QT+ALwKeBNI8cuU9WjAKp6VEQuzY6vBB4ZOe9IdsyZQBlR6sqzD5sgtEc3/Fzq3PMmXiMkU0VARG4EjqnqQRG5vsBrSs6xs56oJSLbgG0Aq1evLvCy/Sb0XHGqdQTWPLoUKeIJvBv4oIjcALwBuEBEvgq8ICIzmRcwAxzLzj8CXDly/SrguaUvqqp7gD0As7OzwR+71/YTd5umDXcyVSFw6jFVBFR1B7ADIPME/lJVbxKRvwW2AvPZ93uzS+4D7hCR2xgkBtcBj037P2UfolC3U1iO0WLiaw3SpWrIU6dOYB7YKyI3A88AHwJQ1SdEZC/wJHAKuKXIzIBP99nBn4Zsm6ZnT0qJgKo+xGAWAFX9H2DDmPN2M5hJcBygegKvyXoFa7UPVki6YrDLxN72q2mqdqoyHmJT2fe8/9nlupJei0Dd/d5CkooANN3orYqfFc8ghMj0WgSmYbExxqKtR575PZ9MCDFyEXAKUWT60GPuyVitHEx2PwGnfaZNH/oMz2RiPRxlGi4CTim8jqB7uAg4pRldhuykjxkRaGtqxZorljLuEbRLqD5iJjE4migqM8oszm/yUSkSvtagHUIPXGZEwKlOXiPp0p6FbZNiwU8dXAQCU3XaLKUKtZSShX2dnpyESREI3QHa7GBVp81Sa6hNLToKPZdu5b5aEnkTIvD4sydaLeG10hCcsxn32XQt72OpDZoQgSKEHB280q1dLIQwzmskIwIh8Uq35pm21qCo8Aq+v0FoOiECZeLIrrmVVpk2a1D083IBCE8nRKCIi963zm9hscqwstBKgZaHIfl0QgScs6lafBWCtusIrIhOKvRCBGZ37Y9tQq9ps6CoC6N924nqXohAbLe4Lm02iuEounZuX6PxeBtC0BUPoO1EtZkFRF2l6Mg06bwYsxchOmtKlYV9oheeQEzyRua8kT1vo8zYdQmL85uCeASxcxTOmSQjAqnFek1smGkhjFkIIASOLUyIwNtWXsiBiu5nm7vTlhEiCx14SN069QVfrt1pTIhAHUJ0tq4kmIY0EVKECA2aJDVPsSkmiXPRcDJ5EXDOpMiIXTXXEMsjiJ0bCU0ob7boa7oI9JDjL51kdtf+Sh0rpEewtLMPO8e0pwulLhKxQ0cXgZ5Sp+GFShbmzZgUvc7azEpKeJ2AU4lQRT9N1RHEHl1TwkUgAawmvRbnNyENv6YXFE0mRNLaRDhw6Ogvgia0Uqfoe46RtAuRLEyhoKhLG9GY8AROvVIsuuyji7dmbh9r5vaZXgQVwiOwTpc2ojEhAl2kaRfeemNa6KEQdAUT4UAZQruJTXXeoQto3a1tEq8sTJPkRGAaw8RJ1cY4Ot1kLZ5LYRrMemVhF2h6u/JCIiAibwa+BPwWg7zNnwJPAXcBa4BF4MOq+rPs/B3AzcBp4FZVvb+QNcaw7oIvtc/ClmLgHkFZynbqpoW/qCfwReDbqvqHIrIcOB/4K+BBVZ0XkTlgDtguIuuBLcA1wBXAAyJytaqebtTylhhtzBZH3lEO7NxYKmsdEgseQdWqyBCUnU1oc7n5VBEQkQuA9wAfAVDVk8BJEdkMXJ+ddjvwELAd2AzcqaovAwsichi4Dni4lqUGsDDKTsNKo4f4y5AtfV5lZxPanH0o4gm8BXgR+IqI/DZwEPgYcJmqHgVQ1aMicml2/krgkZHrj2THzkBEtgHbAJZfflXlN2AdKy76JKrMeZe5xvr77ztFRGAZ8A7go6r6qIh8kYHrP468maKzBgNV3QPsATh3Zl1n80jTRmYLsXOVUafMNRZCA2c8RUTgCHBEVR/Nfr+bgQi8ICIzmRcwAxwbOf/KketXAc81ZXBRQo9AXaoYa4OuJQstPVC0LlNFQFWfF5GfishbVfUpYAPwZPa1FZjPvt+bXXIfcIeI3MYgMbgOeCyE8Xm01dC6VDHWFjE8glDTql0S9qKzAx8FvpbNDPwY+BMG1YZ7ReRm4BngQwCq+oSI7GUgEqeAW1KdGXCax5OF9igkAqr6PWA2508bxpy/G9hd3az+0MfEWddCg9RJtmKwbmWgFYpuSZ5HCvHmOGImC/PaTJ9zNyZEYLjbcOoduin60hgteQR988ZG8VWEJUh55LVKX5Yhl30SVRNPriqKCU/AaYZUpy1jJwvboOz9bvPzMSUC1uderbuMVactq9z3otcIxbdBt35/u4oJEXj82RNTG4q10atLVLmvRcuHi47uXlkYDxMiUARro0TbXkubj1srQ9M2WUoWhsZK+JaMCIwjlhvZtldiUQBCsRhJCNre7NZK1WnyItDEB9KXkcepTxfFOHkRcKqR0mYpTli8TsDp5OjmFMc9AccUbSZAhdcep9bnkNBFwDFFm17J8ElHfa8EdRHAVpGS1anALtP3++0igK0FO31tkH0fjWOSjAhYbSRWCj5iIRSvClxKiCfsOuVJQgQsNxYrBR+x8BLf6lgJQ5MQASefrmyskhJNdkgrnqGLgBMNy+I1On3YdVwEWmRco4+dL7CabylKiPUjCqyd29cLIXARMECb+QLL+ZWyLH0vTS9D7osQeNmwkyR53kuIzjoUgi7jnkABvIAnHmU9lxAbkwwrC4sQO7SrgotAAdoUACvTRlaxLsiWbRuHeRHoW2NPbRRpmxQ7mXVMiMDwuQMxsD6yWMc3CE0fEyLQNpY7fmplyKM2WJ737wJF28a4Uu7ll191bd75yYtAlU5jVQCgXBlyE52uyv2zLKJdpug9L5sUTV4E+l67X5cq96/MvczL7vfhWYspkbwIOOlhIYxpG8thnhcLOU4LWPZY3RNwTNClPMMwV2MleTsN9wQa5JIVy1mc39Sp+vy2aEMA2s4xpCJq7gmUoEzn9vlzexzYuTGJacy2PYlCnoCIfEJEnhCRH4jI10XkDSJysYjsF5Gns+8XjZy/Q0QOi8hTIvL+cObbJQU3MFWG3tbi/KbCo/vwvJRmHNoaRKZ6AiKyErgVWK+qvxKRvcAWYD3woKrOi8gcMAdsF5H12d+vAa4AHhCRq1X1dLB30QJVGo+vAwhPWbEdd36fn4ZcNBxYBpwnIr8GzgeeA3YA12d/vx14CNgObAbuVNWXgQUROQxcBzzcnNnVKOsK1nXHqlzb5TDCcvIv1NOQm9yPIFTbmCoCqvqsiHweeAb4FfAdVf2OiFymqkezc46KyKXZJSuBR0Ze4kh2LAghbkzMxN444QgVy1bxVqp6OFYFYEjsZch5zO7a/2qbyGsbZV775POHD+YdLxIOXMRgdF8L/Bz4ZxG5adIlOcfOuq8isg3YBrB69eqzLihTXFGk06aQEApNXthRxVvpcr5jIYAQ1KEN4SwSDrwPWFDVFwFE5B7gXcALIjKTeQEzwLHs/CPAlSPXr2IQPpyBqu4B9gCcO7NOq3ZS66NLHZpyn33KshyhQoOqhLalyOzAM8A7ReR8ERFgA3AIuA/Ymp2zFbg3+/k+YIuInCsia4F1wGPNmt0Puixw1lmc35Tr0tYlxGvWZaoIqOqjwN3Ad4HHs2v2APPARhF5GtiY/Y6qPgHsBZ4Evg3ckvrMgGOHNmdSQu1ZaI1CswOq+lngs0sOv8zAK8g7fzewu55pTkiazNS3UdQSqwQ3RLLQGl4xmIPV5wM0SZOhxtK9DULcp5ihkbVkYdP0QgRmd+1v5HWqNMTYS0hjFCJ1MZdhLVnYJL0QgZiNsq0lpD4DEJ6UQ4NLViznJ2P+1gsRcGzQhWpIqx5BkUFAPpN/3EXAaY02VvG1EX4tGhWCqiS/n4AvvhmwZm5fY7mPlLG8g49VkvYEuh4Hl3Wf83Ygtjaj0YWQoGuYEIHRh490yc2qy7Dz1rkn1jrcUkGy9Hn3dRdkEyIwSp/X4I+uGEud1PbZg7RDiTq1LeZEIJUGEwKLDz6ty7T/0WfRr0qZxGSRe2tOBCzTpYZYRWxDbArSZ9G3Qi9EoMxo01SjtLyLTlViPXewr7F6W5gUgabnemOMNl0TgJhYjtXHzVClVFlosk7A8ofuOEUIsQw5FCY9AadZirjuKWXxm6Cp0GHSLEiotQZNh2IuAgnQRia/L15VqAKzcfcvhWXILgIJkDdCdzHxGIM27qPVRUdDTOYEnOkc2Lmx82XTbdCWkIbas7AJ3BMwROwprjKzMl7kU54YHkGR++8iQDmXMEQCzcqIXmZWxorNliiyeCvkxiRVPxMPByjnEk47d3bXftPxn9Mek5KFIUKDtRXbnYtAw5SNMd1dnk4X71Go7cyrCIHJcKAv8aa71MWYFn6l6nmFevZh2YegmhQB60UrTW3cEXqb7pCsmdtnxuaUB40QycKyQmBSBFKjiWmmFOf8rdhsQYjqEPJpyEW8Tc8JJI7F0c0pT8xkoXsCiVNkFEw1Zm4DS3sexgoNXAScXmNtz8OQycJxmBCBx589MfHmW0lAObaI/Yi3UITyCJZfdtW1eX8zIQLTCO2uWXIJQ1B2Z6WmG2DZztr0op4yW7E33RaqziQFqSwck3RIQgRCM+5DqdIZLApK7FGw7CYxMTdArXqvmtz4c0hby5B9dqBhfHWf0yRt7FDkIuCYInZiziKhlyG7CEyg6Bx8E3P1lub723zfTjFCegSeE5hAW7G0tfAhdg7BySfUMmT3BBwnIUJUFroIOE5iNB0auAgEwuNqJyRNJgtFNf5myOecf6Euu/DSsX/XV06f+vWxhe+3aNI0LgGOxzaiINFtXX55fqVabE4+f/hgzZd49d6WeY8N/N9XKfN/T504xun/O3GWdpgQARE5oKqzse0oSkr2pmQrpGVvSrZOwsMBx+k5LgKO03OsiMCe2AaUJCV7U7IV0rI3JVvHYiIn4DhOPKx4Ao7jRMJFwHF6jouA4/QcFwHH6TkuAo7Tc/4fd1Hy4gSSS0YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 1000\n",
    "A = 0.001*sparse.random(n, n, 0.0003) + sparse.eye(n)\n",
    "plt.spy(A)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SuperLU at 0x12156ab70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = A.tocsc() # need to convert to CSC form first\n",
    "LU = sla.splu(A)\n",
    "LU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting object stores the factors necessary to compute `A = PLUQ` (`P` permutes rows, and `Q` permutes columns).  It is computed using the [SuperLU library](https://portal.nersc.gov/project/sparse/superlu/).  Typically, you will just use the `solve` method on this object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2129401761290203e-16\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(n)\n",
    "b = A @ x\n",
    "\n",
    "x2 = LU.solve(b)\n",
    "print(np.linalg.norm(x2 - x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can also use the `sla.spsolve` function, which wraps this factorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2129401761290203e-16\n"
     ]
    }
   ],
   "source": [
    "x2 = sla.spsolve(A, b)\n",
    "print(np.linalg.norm(x2 - x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Iterative Methods\n",
    "\n",
    "Sparse iterative methods are another class of methods you can use for solving linear systems built on [Krylov subspaces](https://en.wikipedia.org/wiki/Krylov_subspace).  They only require matrix-vector products, and are ideally used with sparse matrices and fast linear operators.  You can typically learn the theory behind these methods in a numerical linear algebra course - we'll just talk about how to use them. \n",
    "\n",
    "All these methods are meant to solve linear systems: find `x` so that `A @ x = b`, or least squares problems minimizing `norm(A @ x - b)`\n",
    "\n",
    "You can find a list of options in the [documentation for `scipy.sparse.linalg`](https://docs.scipy.org/doc/scipy/reference/sparse.linalg.html#solving-linear-problems).  Here are some common options:\n",
    "\n",
    "* Conjugate Gradient: `sla.cg` for `A` SPD\n",
    "* MINRES: `sla.minres` for `A` symmetric\n",
    "* GMRES: `sla.gmres` for general square `A`\n",
    "* LSQR: `sla.lsqr` for solving least squares problems\n",
    "\n",
    "For example, we can use `gmres` with the same matrix we used for `splu`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00956720325860978\n"
     ]
    }
   ],
   "source": [
    "x2, exit = sla.gmres(A, b, tol=1e-2) # exit code: 0 if successful\n",
    "print(np.linalg.norm(x2 - x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMRES in 0.001146078109741211 sec.\n",
      "spsolve in 0.0008590221405029297 sec.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "x2 = np.empty_like(x)\n",
    "\n",
    "t0 = time.time()\n",
    "x2, exit = sla.gmres(A, b)\n",
    "t1 = time.time()\n",
    "print(\"GMRES in {} sec.\".format(t1 - t0))\n",
    "\n",
    "t0 = time.time()\n",
    "x2 = sla.spsolve(A, b)\n",
    "t1 = time.time()\n",
    "print(\"spsolve in {} sec.\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GMRES also works for other LinearOperator $A$ that can be applied fast, e.g. low rank + sparse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMRES in 0.009238958358764648 sec.\n",
      "sparse in 0.07565903663635254 sec.\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse.linalg import LinearOperator, aslinearoperator\n",
    "import numpy.linalg as la\n",
    "\n",
    "n = 1000\n",
    "onesn = aslinearoperator(np.ones((n,1)))\n",
    "eyem  = aslinearoperator(np.eye(n))\n",
    "Alo = eyem + 0.01*onesn @ onesn.T\n",
    "As  = sparse.csc_matrix(np.eye(n) + np.ones((n,n)))\n",
    "\n",
    "x = np.random.randn(n)\n",
    "b = A @ x\n",
    "\n",
    "t0 = time.time()\n",
    "x2, exit = sla.gmres(Alo, b)\n",
    "t1 = time.time()\n",
    "print(\"GMRES in {} sec.\".format(t1 - t0))\n",
    "\n",
    "t0 = time.time()\n",
    "x2 = sla.spsolve(As, b)\n",
    "t1 = time.time()\n",
    "print(\"sparse in {} sec.\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "Code up a gradient descent algorithm for optimizing $(1/2) x^T A x - b^T x$. Test with diagonal $A$ with diagonal $1,2,3,\\ldots,n$ and $1^2,2^2,\\ldots,n^2$. Test with different step size to see for what step size it converges. Check the rate of convergence. The algorithm should take input $A, b$, stepsize (use constant stepsize), tolerance, and maximum number of iterations. First test with $A$ being $1\\times 1$ with various values of $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(A, b, tau=1, tol=1e-2, max_iter=500):\n",
    "\n",
    "    \n",
    "    x      = np.zeros(A.shape[1])\n",
    "    res    = []\n",
    "    res.append(la.norm(A @ x - b))\n",
    "\n",
    "    for i in range(0, max_iter):\n",
    "        \n",
    "        x = x - tau*(A @ x - b)\n",
    "        \n",
    "        res.append(la.norm(A @ x - b))\n",
    "        \n",
    "        \n",
    "        if res[i]<tol:\n",
    "            break\n",
    "        \n",
    "    \n",
    "    return x, res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.24249966e-01  8.38757626e-04 -4.01093043e-08  1.62886371e-12\n",
      " -1.11022302e-16  0.00000000e+00  2.22044605e-16  1.11022302e-16\n",
      "  0.00000000e+00  0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "A  = np.diag(np.arange(10))\n",
    "x0 = np.random.randn(10)\n",
    "b  = A @ x0\n",
    "\n",
    "[x, res] = gradient_descent(A, b, tol=1e-3, tau=0.1)\n",
    "print(x-x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Direct vs. Iterative Methods\n",
    "\n",
    "There are a couple of trade offs to consider when deciding whether to use sparse direct or iterative algorithms.\n",
    "\n",
    "1.  Are you going to be solving many linear systems with the same matrix `A`?  If so, you can produce a single factorization object using `splu`, and use it to solve many right-hand sides.  Sparse direct probably makes more sense.\n",
    "2. Are you solving a single linear system?  If so, then a single call to an iterative method probably makes mores sense.\n",
    "3. Are you using a fast linear operator that could be expressed as a dense matrix (e.g. sparse plus low-rank)?  Alternatively, would the sparse LU decomposition turn dense because of fill-in?  If so, then iterative methods probably make more sense.\n",
    "4. Do you have a really good preconditioner (see below)?  Then iterative methods probably make more sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preconditioning\n",
    "\n",
    "The speed/effectiveness of iterative methods is often dependent on the existence of a good preconditioner. A preconditioner `M` for a matrix `A` is an \"approximate inverse\" i.e. `M @ A` is close to the identity.  Note if we had an exact inverse, we've solved our problem already.  What we want is to have a matrix `M` which is fast to apply (i.e. also sparse like `A`), which generally isn't possible with an exact inverse.\n",
    "\n",
    "Finding a good preconditioner is a huge field of research, and can be very domain-dependent.  A general-purpose method to obtain a preconditioner is to use an Incomplete LU decomposition (this is an LU factorization that stops when the fill-in gets too large). You can obtain one using `sla.spilu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ILUfact = sla.spilu(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can construct a preconditioner using a `LinearOperator` around the ILU object's solve method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = sla.LinearOperator(\n",
    "    shape = A.shape,\n",
    "    matvec = lambda b: ILUfact.solve(b)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.81655976787981e-15\n"
     ]
    }
   ],
   "source": [
    "x2, exit = sla.gmres(A, b, M=M) # exit code: 0 if successful\n",
    "print(np.linalg.norm(x2 - x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMRES in 0.0014529228210449219 sec.\n",
      "2.7063399512451093e-06\n",
      "preconditioned GMRES in 0.0010259151458740234 sec.\n",
      "6.81655976787981e-15\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "x2, exit = sla.gmres(A, b)\n",
    "t1 = time.time()\n",
    "print(\"GMRES in {} sec.\".format(t1 - t0))\n",
    "print(np.linalg.norm(x2 - x))\n",
    "\n",
    "t0 = time.time()\n",
    "x2, exit = sla.gmres(A, b, M=M)\n",
    "t1 = time.time()\n",
    "print(\"preconditioned GMRES in {} sec.\".format(t1 - t0))\n",
    "print(np.linalg.norm(x2 - x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a higher-precision answer in about the same amount of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalues and Eigenvectors\n",
    "\n",
    "Computing a full eigenvalue decomposition of a sparse matrix or fast linear operator doesn't typically make sense (see the the discussion for sparse direct methods).  However, there are a lot of situations in which we want to compute the eigenvalue-eigenvector pairs for a handful of the largest (or smallest) eigenvalues.  \n",
    "\n",
    "`scipy.sparse.linalg` wraps [ARPACK (ARnoldi PACKage)](https://www.caam.rice.edu/software/ARPACK/), which uses Krylov subspace techniques (like the iterative methods) to compute eigenvalues/eigenvectors using matrix-vector multiplications.  The relevant methods are `sla.eigs` (for general square matrices) and `sla.eigsh` (for symmetric/Hermitian matrices).  There is also a `sla.svds` function for the SVD.\n",
    "\n",
    "Let's look at an example for a linear operator which acts as the matrix of all ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# works on square matrices\n",
    "Afun = lambda X : np.sum(X, axis=0).reshape(1,-1).repeat(X.shape[0], axis=0)\n",
    "\n",
    "m = 10 # linear operator of size 10\n",
    "\n",
    "A = sla.LinearOperator(\n",
    "    shape   = (m,m),\n",
    "    matvec  = Afun,\n",
    "    rmatvec = Afun,\n",
    "    matmat  = Afun,\n",
    "    rmatmat = Afun,\n",
    "    dtype=float   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operator is Hermitian, so we'll use `eigsh`.  By default, `eigsh` will compute the largest magnitude eigenvalues.  You can change which eigenvalues you're looking for using the `which` keyword argument, and the number of eigenvalues using the `k` argument.  See [the documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.eigsh.html#scipy.sparse.linalg.eigsh) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lam, V = sla.eigsh(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.19306142e-17, -1.26876724e-32, -1.22328827e-32,  2.19128633e-32,\n",
       "        1.82828745e-15,  1.00000000e+01])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see there is one eigenvalue with a numerically non-zero value (10).  Let's take a look at the eigenvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.31622777, -0.31622777, -0.31622777, -0.31622777, -0.31622777,\n",
       "       -0.31622777, -0.31622777, -0.31622777, -0.31622777, -0.31622777])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is the vector with constant entries.  This agrees with our understanding of the operator `A`, which can be expressed as the symmetric rank-1 outer product of the vector with 1s in every entry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Linear Algebra\n",
    "\n",
    "In the past decade or two, randomized linear algebra has matured as a topic with lots of practical applications.  To read about the theory, see the 2009 paper by Halko, Martinsson, and Tropp: [Link](http://users.cms.caltech.edu/~jtropp/papers/HMT11-Finding-Structure-SIREV.pdf).\n",
    "\n",
    "SciPy does not (currently) have built-in functions for randomized linear algebra functionality (some languages like Julia do).  Fortunately, these algorithms are very easy to implement without worrying too much about the theory.\n",
    "\n",
    "For simplicity, we'll assume that `A` is symmetric with distinct eigenvectors, so we can limit the discussion to eigenvectors.  A rank-`k` approximation of `A` is an approximation by a rank-`k` outer product.  We can analyitically obtain the optimal rank-`k` approximation by computing a full eigenvalue decomposition of `A` and set all the eigenvalues outside the largest `k` (in magnitude) to 0.  Again, we don't want to actually compute the full eigenvalue decomposition, so we want an algorithm that does this in some provable way.\n",
    "\n",
    "The basic idea is to get an approximation of the range of an operator `A` by applying it to a bunch of random vectors. That is, we compute `A @ X`, where `X` is a matrix with random entries (we think of every column as a random vector). One way to think about the action of `A` is that it \"rotates\" these random vectors preferentially in the direction of the top eigenvectors, so if we look at the most important subspace of the span of the image  `A @ X` (as measured by the svd), we get a good approximation of the most important eigenspace.\n",
    "\n",
    "Randomized algorithms have probabilistic gurarantees.  The statement is roughly that if entries of `X` are iid sub-Gaussian random variables (you can replace \"sub-Gaussian\" with Gaussian), and if we use `k+p` random vectors (`p` is a small constant), we can get close to the top-`k` dimensional eigenspace *with high probability*.  In this case, *close* depends on something called the spectral gap, and *with high probability* means that in order to **not** be close to the desired subspace you would likely need to keep running computations with different random numbers for millions or billions of years before you would observe the algorithm fail.\n",
    "\n",
    "Let's see how this works in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg as la\n",
    "\n",
    "def random_span_k(A, k, p=10, power=0):\n",
    "    \"\"\"\n",
    "    Compute subspace whose span is close to contaning the top-k eigenspace of A\n",
    "    \n",
    "    p = number of dimensions to pad\n",
    "    power : number of times to run a power iteration on the subspace\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "        \n",
    "    X = np.random.randn(n, k+p)\n",
    "    Y = A @ X\n",
    "    \n",
    "    U, s, Vt = la.svd(Y, full_matrices=False)\n",
    "    \n",
    "    for i in range(power):\n",
    "        Y = A @ U\n",
    "        U, s, Vt = la.svd(Y, full_matrices=False)\n",
    "    \n",
    "    \n",
    "    return U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this on a diagonal matrix with entries 0 to `n-1` along the main diagonal.  In this case, the eigenvalues are integers, and the eigenvectors are the standard basis vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<100x100 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 100 stored elements (1 diagonals) in DIAgonal format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 100\n",
    "D = sparse.dia_matrix((np.arange(n), [0]), shape=(n,n))\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "U = random_span_k(D, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lam, V = la.eigh(D.todense())\n",
    "V_true = V[:,-k:]\n",
    "V_true[-k:,:] # should see identity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how well `U` captures each eigenvector.  The distance from this subspace from the `i`th eigenvector is `norm(V[:,i].T*U)`.  Because the eigenvectors are canonical basis vectors, this is just the norm of the `i`th row of `U`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 : 1.6109595277541277e-16\n",
      "01 : 0.006633617127962858\n",
      "02 : 0.023534695279880627\n",
      "03 : 0.02377494077825631\n",
      "04 : 0.026477934577673966\n",
      "05 : 0.027262832798361943\n",
      "06 : 0.038796736650639305\n",
      "07 : 0.04503690478151045\n",
      "08 : 0.05851953567352757\n",
      "09 : 0.09563299793442269\n",
      "10 : 0.06093463168658657\n",
      "11 : 0.07691477197743891\n",
      "12 : 0.09604844251272544\n",
      "13 : 0.065872408785975\n",
      "14 : 0.1208464529012689\n",
      "15 : 0.1701561123423938\n",
      "16 : 0.19869551371606198\n",
      "17 : 0.1645650622860826\n",
      "18 : 0.10884115384520589\n",
      "19 : 0.14211540156282776\n",
      "20 : 0.17732555923544413\n",
      "21 : 0.19157693358863795\n",
      "22 : 0.23139553158260584\n",
      "23 : 0.21779351324283602\n",
      "24 : 0.1418120421968261\n",
      "25 : 0.1145638658509435\n",
      "26 : 0.2245040063249675\n",
      "27 : 0.25214718490486393\n",
      "28 : 0.27935158492507756\n",
      "29 : 0.15633444469518384\n",
      "30 : 0.20807300660261296\n",
      "31 : 0.17977954661300843\n",
      "32 : 0.32569470087758867\n",
      "33 : 0.22135035834364042\n",
      "34 : 0.2552979591899163\n",
      "35 : 0.21554246596410825\n",
      "36 : 0.31268499528071503\n",
      "37 : 0.25788398894521863\n",
      "38 : 0.3020382888968165\n",
      "39 : 0.3290269083301098\n",
      "40 : 0.2536911296384217\n",
      "41 : 0.39712469172867987\n",
      "42 : 0.19855889029154172\n",
      "43 : 0.2489674386814648\n",
      "44 : 0.3523300673088994\n",
      "45 : 0.21328705321605404\n",
      "46 : 0.34188793396602013\n",
      "47 : 0.29562584244205947\n",
      "48 : 0.37917608964902355\n",
      "49 : 0.3828690809722346\n",
      "50 : 0.40840749759467043\n",
      "51 : 0.3733661578123286\n",
      "52 : 0.4969941172529117\n",
      "53 : 0.4693078036678337\n",
      "54 : 0.4902027067646304\n",
      "55 : 0.38364373637550553\n",
      "56 : 0.4051866200503654\n",
      "57 : 0.3781585821721714\n",
      "58 : 0.3883626800818971\n",
      "59 : 0.42243059439573966\n",
      "60 : 0.45319113076086687\n",
      "61 : 0.5257942322832655\n",
      "62 : 0.3569650941972204\n",
      "63 : 0.5754845937080907\n",
      "64 : 0.5124161156097454\n",
      "65 : 0.5609358984537787\n",
      "66 : 0.5335814477199612\n",
      "67 : 0.44211332378798374\n",
      "68 : 0.45956764252219057\n",
      "69 : 0.4372303972022888\n",
      "70 : 0.4449530945958769\n",
      "71 : 0.4224371520652845\n",
      "72 : 0.4061342088046064\n",
      "73 : 0.5401897782940654\n",
      "74 : 0.491871710304369\n",
      "75 : 0.5541262509978716\n",
      "76 : 0.5189077874678637\n",
      "77 : 0.46206567647324825\n",
      "78 : 0.643554632312857\n",
      "79 : 0.5464778312808168\n",
      "80 : 0.37679961593446926\n",
      "81 : 0.6216476978203211\n",
      "82 : 0.4426590690056304\n",
      "83 : 0.4800389142366515\n",
      "84 : 0.5809479729779351\n",
      "85 : 0.485295460668656\n",
      "86 : 0.3937204758027927\n",
      "87 : 0.527801415373641\n",
      "88 : 0.460732058963118\n",
      "89 : 0.546234675976215\n",
      "90 : 0.4398486086462522\n",
      "91 : 0.683335363494225\n",
      "92 : 0.5232857814253857\n",
      "93 : 0.5809837884921637\n",
      "94 : 0.42734462952835356\n",
      "95 : 0.5256156782050099\n",
      "96 : 0.6787035268535148\n",
      "97 : 0.606564202301931\n",
      "98 : 0.6927950993243067\n",
      "99 : 0.6454988797453912\n"
     ]
    }
   ],
   "source": [
    "for i in range(n):\n",
    "    print(\"{:02d} : {}\".format(i, np.linalg.norm(U[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, `U` is closer to the larger eigenvectors, rather than the smaller eigenvectors.\n",
    "\n",
    "We can improve this estimate by running a couple of power iterations on the subspace (the `power` keyword defined above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 : 7.958959704453304e-17\n",
      "01 : 2.916496551801577e-12\n",
      "02 : 1.5488209242149182e-10\n",
      "03 : 1.8873951611515672e-09\n",
      "04 : 1.2589094319306354e-08\n",
      "05 : 4.268103801432829e-08\n",
      "06 : 1.0304216514054878e-07\n",
      "07 : 3.899699240444164e-07\n",
      "08 : 4.631287997911403e-07\n",
      "09 : 1.3689886608365438e-06\n",
      "10 : 1.8855366507575138e-06\n",
      "11 : 7.43169878555308e-06\n",
      "12 : 8.564345553369577e-06\n",
      "13 : 1.6534197549019153e-05\n",
      "14 : 2.544284797788348e-05\n",
      "15 : 3.908658770660757e-05\n",
      "16 : 3.8694352678965526e-05\n",
      "17 : 5.092830171274271e-05\n",
      "18 : 7.324705386486573e-05\n",
      "19 : 0.00011961353563798645\n",
      "20 : 0.00010259615750577138\n",
      "21 : 0.00017488264846066826\n",
      "22 : 0.0002930332274596114\n",
      "23 : 0.0003098032324709351\n",
      "24 : 0.0004694779579575787\n",
      "25 : 0.0010485166781785556\n",
      "26 : 0.0006068797966368376\n",
      "27 : 0.0010294948662544162\n",
      "28 : 0.0011032539999480339\n",
      "29 : 0.0013197789607760754\n",
      "30 : 0.0016159140133598271\n",
      "31 : 0.0021465604093732662\n",
      "32 : 0.0034442630741350843\n",
      "33 : 0.0036418266417170917\n",
      "34 : 0.0051843610005941915\n",
      "35 : 0.005135032147916564\n",
      "36 : 0.007043228791337847\n",
      "37 : 0.008240280713454173\n",
      "38 : 0.0072887978151776256\n",
      "39 : 0.010628527663560858\n",
      "40 : 0.013040844342671568\n",
      "41 : 0.011698951628082798\n",
      "42 : 0.01604400537350304\n",
      "43 : 0.017887956592585255\n",
      "44 : 0.022656652180513197\n",
      "45 : 0.019624809780314455\n",
      "46 : 0.0187017537274577\n",
      "47 : 0.03523773909514776\n",
      "48 : 0.026969087578270868\n",
      "49 : 0.034203958915582276\n",
      "50 : 0.031444492617784356\n",
      "51 : 0.06321254685884822\n",
      "52 : 0.07978620610596864\n",
      "53 : 0.0567260425784685\n",
      "54 : 0.04582300401538413\n",
      "55 : 0.07140176842837406\n",
      "56 : 0.08535270657128678\n",
      "57 : 0.09157396919858353\n",
      "58 : 0.0947192161994437\n",
      "59 : 0.1141034224434171\n",
      "60 : 0.16494632814580232\n",
      "61 : 0.11657114399064043\n",
      "62 : 0.14931934356898374\n",
      "63 : 0.13485768028339962\n",
      "64 : 0.3038086010965716\n",
      "65 : 0.18033696599195598\n",
      "66 : 0.3774980466159308\n",
      "67 : 0.23934467009976498\n",
      "68 : 0.39437729016276135\n",
      "69 : 0.4055794734334976\n",
      "70 : 0.25939002030059544\n",
      "71 : 0.35271972332706053\n",
      "72 : 0.35173346890273666\n",
      "73 : 0.368943441321892\n",
      "74 : 0.44115105069635474\n",
      "75 : 0.2875324552483845\n",
      "76 : 0.5857604847826767\n",
      "77 : 0.601448488020223\n",
      "78 : 0.6090962251166495\n",
      "79 : 0.581800419328311\n",
      "80 : 0.6588617855053556\n",
      "81 : 0.5726365858240263\n",
      "82 : 0.5892249509547143\n",
      "83 : 0.5953944492332695\n",
      "84 : 0.71157208362805\n",
      "85 : 0.6557796887851262\n",
      "86 : 0.8085046662957783\n",
      "87 : 0.6420100080779907\n",
      "88 : 0.6686188869396887\n",
      "89 : 0.7758582924346679\n",
      "90 : 0.7373155062202128\n",
      "91 : 0.8704403663720611\n",
      "92 : 0.8634945719500866\n",
      "93 : 0.8710654526853401\n",
      "94 : 0.9187223193118006\n",
      "95 : 0.8495136210493307\n",
      "96 : 0.8835188358930772\n",
      "97 : 0.9306946034187981\n",
      "98 : 0.819805363398525\n",
      "99 : 0.9276971457839125\n"
     ]
    }
   ],
   "source": [
    "U = random_span_k(D, k, power=5)\n",
    "for i in range(n):\n",
    "    print(\"{:02d} : {}\".format(i, np.linalg.norm(U[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Download a couple of test matrices from the UFlorida Sparse Matrix collection [Link](https://sparse.tamu.edu/)\n",
    "For, example, use `mnist_test_norm_10NN` [Link](https://sparse.tamu.edu/ML_Graph/mnist_test_norm_10NN) which would probably be too large to store on your computer as a dense matrix.\n",
    "\n",
    "For each square matrix:\n",
    "1. Solve a random linear system using `splu`\n",
    "2. Solve a random linear system using either `minres` or `gmres` (which one should you use?)\n",
    "3. Compute the largest magnitude eigenvector using `eigs` or `eigsh` (which one should you use?)\n",
    "\n",
    "Find a non-square matrix and\n",
    "1. Solve a random least squares problem using `lsqr`\n",
    "2. Compute the largest singular vectors using `svds`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pycourse)",
   "language": "python",
   "name": "pycourse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
